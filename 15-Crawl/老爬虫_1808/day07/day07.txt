Day06回顾
1. 多线程爬虫
  1. URL队列 : put(url)
  2. RES队列 : 从URL队列中get()发请求,put(html)
  3. 创建多个线程
2. 队列
  1. 先把html放到解析队列,然后再取 
    while True:
        if not resQueue.empty():
            ...get()
     else:
        break
  2. 一边放html,一边取html进行解析
    while True:
        try:
            ...get(block=True,timeout=2)
            ...
        except:
            break
3. BeautifulSoup : HTML/XML解析库
  1. 使用流程
    from bs4 import BeautifulSoup
    soup = BeautifulSoup(html,'lxml')
    rObj = soup.find()
    rList = soup.find_all()
  2. 支持解析库
    lxml  html.parser  xml 
  3. find_all('div',attrs={'class':'abc'})
  4. 节点.get_text() : 文本(包括子节点)
     节点.string     : 文本(不包括子节点)
4. scrapy框架 : 异步处理框架
  1. 组成
    Engine,Scheduler,Downloader,Spider,Item Pipeline,Downloader Middlewares,Spider Middlewares
  2. 运行流程
    1. Engine向Spider索要URL(第1个爬取)
    2. 交给Scheduler入队列
    3. Scheduler出队列,通过Downloader Middlewares交给Downloader
    4. 下载完成,通过Spider Middlewares给Spider
    5. Spider做数据提取
      把数据交给 Item Pipeline
      把需要跟进URL交给 Scheduler入队列
    6. 当Scheduler中没有任何Request请求后,程序结束
5. 创建项目流程
  1. scrapy startproject Lianjia
  2. items.py : 定义爬取数据结构
     class LianjiaItem():
         name = scrapy.Filed()
         price = scrapy.Filed()
  3. scrapy genspider lianjia lianjia.com
  4. lianjia.py
     import scrapy
     class LianjiaSpider(scrapy.Spiders):
         name = "lianjia"
         allowed_domains = ['lianjia.com']
         start_urls = ['http://....']
         
         def parse(self,response):
             pass 
  5. Pipelines.py 
     class LianjiaPipeline(object):
        def process_item(self,item,spider):
             pass 
  6. settings.py
    ROBOTSTXT_OBEY = False 
    USER_AGENT = 
    DEFAULT_REQUEST_HEADERS = {}
    ITEM_PIPELINES = {
      'Lianjia.pipelines.LianjiaPipeline':1
    }
  7. scrapy crawl lianjia
6. pycharm运行爬虫项目
  1. begin.py(scrapy.cfg同级目录)
     from scrapy import cmdline 
     cmdline.execute('scrapy crawl lianjia'.split())
  2. File - settings - Project InterPreter
****************************************
1. yield回顾
  1. 作用 : 把1个函数当做1个生成器使用
  2. 特点 : 让函数暂停,等待下1次调用
2. Csdn
  1. 网址https://blog.csdn.net/XiaoYi_Eric/article/details/85559389
  2. 标题 发表时间 阅读数
   标题://h1[@class="title-article"]/text()
   时间://span[@class="time"]/text()
   数量://span[@class="read-count"]/text()
3. 知识点
  1. extract() : 获取选择器对象中的文本内容
    response.xpath('')
    结果: [<selector ....,data='文本内容'>]
    response.xpath('').extract()[0]
    结果: '文本内容'
  2. 爬虫程序中,start_urls必须为列表
    start_urls = []
  3. pipelines.py中必须有1个函数叫:
     class CsdnPipeline(object):
        def process_item(self,item,spider):
            pass 
4. 腾讯招聘项目(数据持久化存储)
  1. 网站 : https://hr.tencent.com/position.php?&start=0
  2. Xpath匹配
    基准Xpath表达式 ：
    //tr[@class="even"]|//tr[@class="odd"]
    职位名称 : ./td[1]/a/text()
    职位类别 : ./td[2]/text()
    招聘人数 : ./td[3]/text()
    工作地点 : ./td[4]/text()
    发布时间 : ./td[5]/text()
    职位链接 : ./td[1]/a/@href
5. 日志级别及保存日志文件
   LOG_LEVEL = ''
   LOG_FILE = '文件名.log'
   5层警告级别
     1. CRITICAL 严重错误
     2. ERROR    一般错误
     3. WARNING  警告信息
     4. DEBUG    调试信息
     5. INFO     一般信息
6. 保存为csv或json文件
  1. scrapy crawl tengxun -o Tencent.json
    json文件编码问题 : 在settings.py中添加
       FEED_EXPORT_ENCODING = 'utf-8'
  2. scrapy crawl tengxun -o Tencent.csv
    csv文件出现空行的解决方法(修改源文件exporters.py):
    路径 C:\ProgramData\Anaconda3\Lib\site-packages\scrapy
    在exporters.py中搜索 csv,找csv的类,在 
      self.stream = io.TextIOWrapper(
            file,
            newline = ""  ## 添加此参数
7. Daomu
  1. URL : http://www.daomubiji.com/dao-mu-bi-ji-1 
  2. 目标
    基准: 
      response.xpath('//article/a/text()').extract()
      # ["七星鲁王 第一章 血尸","... .. .."]
      i = 0 
      for r in []:
        标题
        章节数
        章节名称
        链接  '//article/a/@href'[i]
        i += 1
        yield item    
      

1. 创建项目Daomu
2. 创建爬虫文件 daomu 
3. 在pipelines.py中创建2个类(mysql和mongo)
4. 在settings.py中设置好管道(3个管道)
5. 在settings.py中设置相关选项(级别 DEBUG)






  