Day07回顾
1. response.xpath('')
   结果 : 列表,选择器对象
   extract() : 提取文本内容,将列表中所有元素序列化为Unicode字符串
2. MongoDB持久化存储
  1. settings.py设置相关变量
     MONGO_HOST = ''
     MONGO_PORT = 27017
     MONGO_DB = ''
     MONGO_SET = ''
  2. pipelines.py定义相关类
     from 项目名.settings import *
     class MongoPipeline(object):
        def __init__(self):
            pass 
        def process_item(self,item,spider):
            return item 
        def close_spider(self,spider):
            pass 
  3. settings.py设置ITEM_PIPELINES
     ITEM_PIPELINES = {
       '项目名.pipelines.类名' : 200,
     }
3. settings.py常用变量
   LOG_LEVEL = ''
   LOG_FILE = 'XXX.log'
   FEED_EXPORT_ENCODING = 'utf-8'
4. 日志级别
   CRITICAL : 严重错误
   ERROR    : 一般错误
   WARNING  : 警告信息
   DEBUG    : 调试信息
   IINFO    : 一般信息
5. 保存为csv或json文件
  1. scrapy crawl 爬虫名 -o XXX.json
  2. scrapy crawl 爬虫名 -o XXX.csv
  3. 指定导出编码 : FEED_EXPORT_ENCODING = 
  4. 解决csv文件空行问题
    1. 找exporters.py改一下源码
      找到csv的类,在files下面添加参数:
      newline="",
6. scrapy.Request(url,callback=解析方法名)
******************************************
Day08笔记
1. scrary shell使用
  1. scrapy shell URL地址
  2. response.text : string类型
  3. response.body : bytes类型
  4. request.headers : 请求头(字典)
  5. request.meta : 定义代理等参数相关信息{}
  6. request.xpath('')
2. scrapy.Request()常用参数
  1. url : URL地址
  2. callback : 指定解析函数
  3. headers : 请求头(不需要)
  4. meta : 字典
     1. 定义代理等相关参数信息
     2. 在不同请求之间传递数据
  5. dont_filter : 是否忽略域组限制
     默认为False : 检查allowed_domains
  6. encoding : 默认utf-8,不用配置
3. 下载器中间件(随机User-Agent)
  1. settings.py(少量User-Agent切换)
    1. USER_AGENT = ''
    2. DEFAULT_REQUEST_HEADERS = {}
  2. middlewares.py设置中间件
    1. 项目目录中新建useragents.py存放大量User-Agent的列表
    2. middlewares.py定义相关类
      class RandomUserAgent(object):
        def process_request(self,request,.)
            request.headers['User-Agent'] = random.choice(...)
    3. settings.py
      DOWNLOADER_MIDDLEWARES = {
        '项目名.middlewares.类名':200,
      }
4. 下载器中间件(设置随机代理)
  1. 新建文件(proxies.py)
  2. middlewares.py(新建类)
  3. settings.py(启用中间件)
5. 升级Scrapy方法
  scrapy -v
  管理员 : Anaconda Prompt
  升级pip
    python -m pip install --upgrade pip 
  升级Scrapy
    pip install --upgrade Scrapy
6. CrawlSpider类
  1. Spider的派生类
    from scrapy.spiders import CrawlSpider
    定义了一些规则(Rule)来提供跟进链接,从爬取的网页中提取链接并继续爬取
  2. 提取链接的流程(LinkExtractor)
    1. scrapy shell 腾讯第一页的URL地址
    2. from scrapy.linkextractors import LinkExtractor
    3. LinkExtractor(allow='').extract_links(response)
  3. 创建爬虫文本模板(CrawlSpider类)
    scrapy genspider -t crawl 爬虫名 域名


7. 腾讯招聘子页面爬取
  1. items.py去定义要爬取的数据结构
  2. 爬虫文件.py逻辑(不同解析函数间使用meta属性传递数据)
    def parse1():
      yiled scrapy.Request(url,callback=..,
                         meta={"item":item})
    def parse2():
      item = response.meta['item']
8. 分布式部署
  1. scrapy_redis模块
    Anaconda Prompt : 
      pip install scrapy_redis
  2. Redis 
************************************
9. 分布式原理 : 多台机器共享1个爬取队列 
10.实现分布式
  scrapy_redis(重写scrapy调度器)
11. 为什么使用redis
  1. Redis是非关系型数据库,key-value形式存储,结构灵活
  2. Redis集合,存储每个request的指纹(加密)
12. redis安装
  1. Windows
    1. 直接下载
    2. 启动并连接
      1. 服务端启动 : cmd终端 : redis-server
      2. 客户端连接 : redis-cli -h IP地址
    3. 安装图形界面管理工具
      RedisDesktopManger
      新建连接 + Connect to Redis Server 
  2. Ubuntu
    1.安装 
      sudo apt-get install redis-server
    2.启动 
      redis-server
    3.客户端连接
      redis-cli -h IP地址
13. scrapy_redis安装
  Anaconda Prompt
    pip install scrapy_redis






    




基准xpath:xpath('//ul[@class="squareli"]')
# 结果 ：L [工作职责对象,工作要求对象]
L[0].xpath('.//li').extract()
''.join(['职责1','职责2','职责3','职责4'])





for r in L:
    r.xpath('.//li')




名称
人数
...
...
岗位职责
岗位要求







pip install scrapy_redis



